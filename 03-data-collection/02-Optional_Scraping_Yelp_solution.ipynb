{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73GKjaLs2tqX"
   },
   "source": [
    "# Scraping Yelp\n",
    "\n",
    "The aim of this exercise is to allow a user to make an automatic search on <a href=\"https://www.yelp.fr/\" target=\"_blank\">Yelp</a> and store the results in a `.json` file. You will be guided through the different steps: making a form request with search keywords, parsing the search results, crawling all the result pages and storing the results into a file.\n",
    "\n",
    "⚠ **As scrapy is not made to launch several crawler processes in the same script, you will have to restart your notebook's kernel before completing each question!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig9e7f2V2tqn"
   },
   "source": [
    "1. Create a class `YelpSpider(scrapy.Spider)` with `start_urls = ['https://www.yelp.fr/']`. In this class, define a `parse(self, response)` method that automatically fills Yelp's homepage form with : \"restaurant japonais\" as search keywords and \"Paris\" as search location. Then, define another method `after_search(self, response)` that parses the first page of results, and yields the name and url of each search result. Finally, declare a `CrawlerProcess` that will store the results in a file named `\"restaurant_japonais-paris.json\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2cYdufu2tqq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuP96e1E2tqt"
   },
   "outputs": [],
   "source": [
    "class YelpSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"yelp\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['https://www.yelp.fr/']\n",
    "\n",
    "    # Parse function for form request\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to make a search in Paris\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'find_desc': 'restaurant japonais', 'find_loc': 'paris'},\n",
    "            callback=self.after_search\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_search(self, response):\n",
    "        \n",
    "        results = response.css('h4 a')\n",
    "        \n",
    "        for r in results:\n",
    "            yield {\n",
    "                'name': r.css('::text').get(),\n",
    "                'url': \"https://www.yelp.fr\" + r.attrib[\"href\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCJlMWsU2tqw",
    "outputId": "e44c436e-3dda-4c6b-add4-f474f7146ffd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-03 15:34:10 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: scrapybot)\n",
      "2021-07-03 15:34:10 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.9.2 (default, Feb 24 2021, 13:30:36) - [Clang 12.0.0 (clang-1200.0.32.29)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform macOS-10.15.4-x86_64-i386-64bit\n",
      "2021-07-03 15:34:10 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-07-03 15:34:10 [scrapy.extensions.telnet] INFO: Telnet Password: 709767298101ac95\n",
      "2021-07-03 15:34:10 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-07-03 15:34:10 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-07-03 15:34:10 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-07-03 15:34:10 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-07-03 15:34:10 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-07-03 15:34:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-07-03 15:34:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-07-03 15:34:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-07-03 15:34:13 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: results/restaurant_japonais-paris.json\n",
      "2021-07-03 15:34:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 996,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 78729,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 3.015736,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 7, 3, 13, 34, 13, 985302),\n",
      " 'httpcompression/response_bytes': 445249,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 76197888,\n",
      " 'memusage/startup': 76197888,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2021, 7, 3, 13, 34, 10, 969566)}\n",
      "2021-07-03 15:34:13 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"restaurant_japonais-paris.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('results/'):\n",
    "        os.remove('results/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'results/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(YelpSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMKL5t2N2tq4"
   },
   "source": [
    "2. Once you've managed to get the first page's results in `restaurant_japonais-paris.json`, complete the `after_search(self,response)` method to crawl the different result pages, such that all the search results will be stored in the file `\"restaurant_japonais-paris.json\"`. Restart your notebook's kernel, execute the new `CrawlerProcess` and check that all the search results (and not only the first page) are now stored in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaQw2odH2tq6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_juU4Bgy2tq9"
   },
   "outputs": [],
   "source": [
    "class YelpSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"yelp\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['https://www.yelp.fr/']\n",
    "\n",
    "    # Parse function for form request\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to make a search in Paris\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'find_desc': 'restaurant japonais', 'find_loc': 'paris'},\n",
    "            callback=self.after_search\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_search(self, response):\n",
    "        \n",
    "        results = response.css('h4 a')\n",
    "        \n",
    "        for r in results:\n",
    "            yield {\n",
    "                'name': r.css('::text').get(),\n",
    "                'url': \"https://www.yelp.fr\" + r.attrib[\"href\"]\n",
    "            }\n",
    "            \n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('a.next-link').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tid8CQFE2trA",
    "outputId": "44ecbb42-2aed-4479-c504-4fed80519432"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 14:24:22 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-09-01 14:24:22 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2020-09-01 14:24:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-09-01 14:24:22 [scrapy.extensions.telnet] INFO: Telnet Password: d744b107c4cf10b2\n",
      "2020-09-01 14:24:22 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-09-01 14:24:23 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-09-01 14:24:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-09-01 14:24:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-09-01 14:24:47 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-09-01 14:24:47 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-09-01 14:24:47 [scrapy.extensions.feedexport] INFO: Stored json feed (240 items) in: results/restaurant_japonais-paris.json\n",
      "2020-09-01 14:24:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 12467,\n",
      " 'downloader/request_count': 10,\n",
      " 'downloader/request_method_count/GET': 10,\n",
      " 'downloader/response_bytes': 702946,\n",
      " 'downloader/response_count': 10,\n",
      " 'downloader/response_status_count/200': 9,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 24.157405,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 9, 1, 14, 24, 47, 172858),\n",
      " 'item_scraped_count': 240,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75759616,\n",
      " 'memusage/startup': 75759616,\n",
      " 'request_depth_max': 8,\n",
      " 'response_received_count': 9,\n",
      " 'scheduler/dequeued': 10,\n",
      " 'scheduler/dequeued/memory': 10,\n",
      " 'scheduler/enqueued': 10,\n",
      " 'scheduler/enqueued/memory': 10,\n",
      " 'start_time': datetime.datetime(2020, 9, 1, 14, 24, 23, 15453)}\n",
      "2020-09-01 14:24:47 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"restaurant_japonais-paris.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('results/'):\n",
    "        os.remove('results/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'results/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(YelpSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjqeEpwf2trG"
   },
   "source": [
    "Congrats, you've just made the proof of concept of making an automated search on Yelp with Scrapy! Now, let's improve the script such that it will allow the user to make any search at any location 😎\n",
    "\n",
    "3. Use python's `input()` function to ask the user which keywords and location he would like to use, and save them into two variables: `search_keywords` and `search_location`. Then, change the `parse(self, response)` method such that it fills Yelp's form with user-defined keywords and location. Finally, change the `CrawlerProcess` such that it stores the results in a file named with the following format : `search_keywords-location.json`. \n",
    "\n",
    "Try your search engine with different keywords and locations ✌️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_QmZNay2trJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPEdFpUi2trK",
    "outputId": "25c5963b-469e-40e5-a595-8c2287c755c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in the automated Yelp search engine !\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your search keywords :  spa\n",
      "Please enter the name of the city :  Grenoble\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome in the automated Yelp search engine !\")\n",
    "\n",
    "search_keywords = input(\"Please enter your search keywords : \")\n",
    "search_location = input(\"Please enter the name of the city : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y28OiUfI2trM"
   },
   "outputs": [],
   "source": [
    "class YelpSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"yelp\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['https://www.yelp.fr/']\n",
    "\n",
    "    # Parse function for form request\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to make a search\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'find_desc': search_keywords, 'find_loc': search_location},\n",
    "            callback=self.after_search\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_search(self, response):\n",
    "        \n",
    "        results = response.css('h4 a')\n",
    "        \n",
    "        for r in results:\n",
    "            yield {\n",
    "                'name': r.css('::text').get(),\n",
    "                'url': \"https://www.yelp.fr\" + r.attrib[\"href\"]\n",
    "            }\n",
    "            \n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('a.next-link').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KVWNMpP2trO",
    "outputId": "b5142cc7-8b9e-441d-a50d-5ed747bb21fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 14:31:45 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-09-01 14:31:45 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2020-09-01 14:31:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-09-01 14:31:45 [scrapy.extensions.telnet] INFO: Telnet Password: f7dddfab2d16ca6e\n",
      "2020-09-01 14:31:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-09-01 14:31:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-09-01 14:31:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-09-01 14:31:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-09-01 14:31:50 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-09-01 14:31:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-09-01 14:31:50 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: results/spa-Grenoble.json\n",
      "2020-09-01 14:31:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1986,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 121408,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 4.849391,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 9, 1, 14, 31, 50, 872261),\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75993088,\n",
      " 'memusage/startup': 75993088,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2020, 9, 1, 14, 31, 46, 22870)}\n",
      "2020-09-01 14:31:50 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = search_keywords.replace(\" \", \"_\") + \"-\" + search_location + \".json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('results/'):\n",
    "        os.remove('results/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'results/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(YelpSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02-Optional_Scraping_Yelp_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
