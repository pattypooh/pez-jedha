{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3925fe8-76db-471f-8261-4033bcd297fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction to Data Warehousing\n",
    "\n",
    "The concept of Data Warehousing originated at IBM in the 80's. The goal of the initial research was to provide a framework to transfer data from operational systems to business intelligence departments, avoiding the cost and technical challenges of high redundancy.\n",
    "\n",
    "## What will you learn in this course? üßêüßê\n",
    "This lecture will introduce the concept of data warehousing and why do we need it. Here's the outline:\n",
    "* Why Analysts cannot work directly on business databases\n",
    "* Data Warehouse VS Data Lake\n",
    "* Data Warehouse VS traditional databases\n",
    "    * Key differences*\n",
    "* Cloud vendors\n",
    "* Amazon Redshift\n",
    "    * Reading from Redshift onto a PySpark DataFrame\n",
    "    * Writing to Redshift from PySpark DataFrame\n",
    "\n",
    "\n",
    "## Why Analysts cannot work directly on business databases ü§îü§î\n",
    "\n",
    "Business databases must stay clean at all cost: allowing Data Analysis or Data Scientist to access it introduces a breach \n",
    "\n",
    "Moreover, most of the time, unstructured data (ie, not stored in any kind of databases) is required to do performant analysis. \n",
    "\n",
    "A Warehousing solution allows the company to aggregate and store its data needed for analysis, without altering the databases used for operations.\n",
    "\n",
    "## Data Warehouse VS Data Lake üóÑÔ∏èüÜöüåä\n",
    "\n",
    "You often hear both when discussing Big Data, however they are very different.\n",
    "\n",
    "Data Lakes are a big pool of raw data, with no defined purposes: we store this unstructured data in prevision of future usage.\n",
    "\n",
    "Data Warehouse holds **processed** and **structured** data, ready to be used for advanced analytics. \n",
    "\n",
    "Most of the time, data that ends up in the Warehouse was previously stored in the Lake. \n",
    "\n",
    "- Step 1: Data is collected and stored in its raw form in a Data Lake\n",
    "- Step 2: Data is extracted from the Lake, cleaned and processed\n",
    "- Step 3: Data is loaded in the warehouse, ready to be queried.\n",
    "\n",
    "## Data Warehouse VS traditional databases üóÑÔ∏èüóÑÔ∏èüóÑÔ∏èüÜöüóÑÔ∏è\n",
    "\n",
    "Roughly, a Data Warehouse **is** a relational database. It's just a little more than that.\n",
    "\n",
    "### Key differences üîë\n",
    "\n",
    "1. The Warehouse can hold data from many databases\n",
    "2. Any data stored in the Warehouse is stored for **analytics purposes only**\n",
    "3. Data within a warehouse has been processed to simplify the analysis, and avoid the need for  SQL queries that spread on 300 lines\n",
    "4. Whereas databases are optimized for extracting rows (or observations), data warehouses are optimized to have a performance boost on columns (or fields).\n",
    "\n",
    "In a nutshell: warehouses are optimized for performant analysis.\n",
    "\n",
    " **A warehouse is the perfect candidate for `LOAD` destination in ETL pipelines.**\n",
    "\n",
    "## Cloud vendors ‚òÅÔ∏è‚òÅÔ∏è\n",
    "\n",
    "- BigQuery, owned by Google, and part of the Google Cloud Platform\n",
    "- Redshift, owned by Amazon and part of the AWS platform\n",
    "- Snowflake\n",
    "- ...\n",
    "\n",
    "As always when choosing between different vendors, the cost structure is one the most important aspects to check. For instance, BigQuery storage is **much** cheaper than Redshift, but querying data on Redshift is **free** whereas it costs about 5 dollars/TB on BigQuery. Depending on your need, one solution might be more suitable than the other.\n",
    "\n",
    "## Amazon Redshift üî¥üî¥\n",
    "\n",
    "Redshift is the Data Warehousing solution from Amazon Web Services. As every services of the AWS family, Redshift is **Cloud-based**: you only pay for the compute and storage, and you don't have to take care of maintenance costs, or scaling the hardware to support an increasing load.\n",
    "\n",
    "### Reading from Redshift onto a PySpark DataFrame üî¥‚û°‚ú®\n",
    "\n",
    "```\n",
    "REDSHIFT_USER = 'YOUR_REDSHIFT_USERNAME'\n",
    "REDSHIFT_PASSWORD = 'YOUR_REDSHIFT_PASSWORD'\n",
    "\n",
    "redshift_path_full = \"JDBC_LINK\" # don't forget to replace \"redshift\" by \"postgresql\"\n",
    "REDSHIFT_TABLE = 'NAME_OF_THE_TABLE'\n",
    "\n",
    "properties = {\"user\": REDSHIFT_USER, \"password\": REDSHIFT_PASSWORD, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "table = sqlContext.read.jdbc(url=REDSHIFT_URL, table=REDSHIFT_TABLE, properties=properties)\n",
    "```\n",
    "        \n",
    "Although this can be useful, it is also possible to query your database using the redshift query editor directly, which is most likely what data analysts and business analysts would be doing in a real-life context.\n",
    "\n",
    "### Writing to Redshift from PySpark DataFrame ‚ú®‚û°üî¥\n",
    "```\n",
    "REDSHIFT_USER = 'YOUR_REDSHIFT_USERNAME'\n",
    "REDSHIFT_PASSWORD = 'YOUR_REDSHIFT_PASSWORD'\n",
    "\n",
    "redshift_path_full = \"JDBC_LINK\" # don't forget to replace \"redshift\" by \"postgresql\"\n",
    "REDSHIFT_TABLE = 'NAME_OF_THE_TABLE'\n",
    "```\n",
    "As written in this [tutorial](https://github.com/databricks/spark-redshift/tree/master/tutorial), there are several modes you can choose from when loading data from Spark to Redshift. \n",
    "\n",
    "The 4 `mode` to choose from are:\n",
    "  - `overwrite`: drop the table if it exists, then load the data in a new one\n",
    "  - `append`: create the table if it does not exists, else append the data to the existing table\n",
    "  - `error` (default) : create the table or raise an error if it exists\n",
    "  - `ignore`: same as `overwrite`, but does nothing if table already exists\n",
    "\n",
    "We ask you to use the `overwrite` mode here. Also, you need to set the option `tempformat` to `csv` because the default Avro format does not allow non letter characters (such as `_`) in columns names.\n",
    "```\n",
    "mode = \"overwrite\"\n",
    "properties = {\"user\": REDSHIFT_USER, \"password\": REDSHIFT_PASSWORD, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "df_clean.write.jdbc(url=redshift_path_full, table=REDSHIFT_TABLE, mode=mode, properties=properties)\n",
    "```\n",
    "As Spark uses an S3 bucket to store the intermediary files, both Spark and Redshift needs to have access to the S3 bucket.\n",
    "\n",
    "‚Üí Ensure that the Redshift cluster has assumed an IAM role that gives it access to the `tempdir` S3 bucket (or use `forward_spark_s3_credentials` option)\n",
    "\n",
    "‚Üí By default, Spark uses the Avro format as an intermediary storage in S3. Using CSV can significantly improve loading performance, and also allow columns to have names with characters other than ASCII letters.\n",
    "\n",
    "‚Üí By default, every `string` column is loaded as a 256-byte length `VARCHAR` to Redshift. To gain performance or flexibility, it is possible to edit the default behavior by giving a `redshift_type` metadata to the DataFrame's column. See docs below for implementation in Scala and Python.\n",
    "\n",
    "## Ressources üìöüìö\n",
    "\n",
    "[A nice article on Alooma's blog]([https://www.alooma.com/blog/database-vs-data-warehouse](https://www.alooma.com/blog/database-vs-data-warehouse))\n",
    "[Amazon Redshift](https://docs.databricks.com/data/data-sources/aws/amazon-redshift.html#setting-a-custom-column-type)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-Introduction_to_Data_Warehousing",
   "notebookOrigID": 4407832410620818,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
